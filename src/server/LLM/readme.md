### Issues:
- How do we handle complex defenses that run the output of one llm against another llm?
- How should the queue system work for processing multiple attacks at one time?
- Should there be a timeout?
    - If so how can we make sure that the request is stopped?
- Should we use streaming?
- How should testing (attacking your own defense) be prioritized compared to attacks?


### Resources:
Ollama API: https://github.com/ollama/ollama/blob/main/docs/api.md